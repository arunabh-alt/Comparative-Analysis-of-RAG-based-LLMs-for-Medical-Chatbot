import matplotlib.pyplot as plt
import pandas as pd

# Data for Flan-T5-Large
flan_t5_large_data = {
    'step': [
        100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600,
        1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000,
        3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400,
        4500, 4600, 4700, 4800, 4900, 5000, 5100, 5200, 5300, 5400, 5500, 5600, 5700, 5800,
        5900, 6000, 6100, 6200, 6300, 6400, 6500, 6600, 6700, 6800, 6900, 7000, 7100, 7200,
        7300, 7400, 7500, 7600, 7700, 7800, 7900, 8000, 8100, 8200, 8300, 8400, 8500, 8600,
        8700, 8800, 8900, 9000, 9100, 9200, 9300, 9400, 9500, 9600, 9700, 9800, 9900, 10000
    ],
    'loss': [
        1.5796, 1.5587, 1.5564, 1.4887, 1.4822, 1.4375, 1.4167, 1.3488, 1.3361, 1.3263, 1.3105,
        1.2922, 1.2298, 1.1850, 1.1324, 1.1526, 1.1235, 1.1198, 1.0862, 1.0773, 1.0275, 1.0393,
        0.9720, 1.0178, 1.0158, 0.9558, 0.9462, 0.9347, 0.9755, 0.9403, 0.9400, 0.9110, 0.9409,
        0.8681, 0.8557, 0.8641, 0.8464, 0.8315, 0.8501, 0.8058, 0.8090, 0.7821, 0.7900, 0.7541,
        0.7627, 0.7485, 0.7590, 0.7617, 0.7190, 0.7488, 0.7363, 0.7309, 0.7387, 0.7238, 0.7236,
        0.7294, 0.7032, 0.7011, 0.7212, 0.6912, 0.6758, 0.6703, 0.6940, 0.6571, 0.6846, 0.6842,
        0.6843, 0.6774, 0.6653, 0.6531, 0.6467, 0.6689, 0.6525, 0.6667, 0.6539, 0.6419, 0.6438,
        0.6657, 0.6420, 0.6638, 0.6717, 0.6750, 0.6587, 0.6655, 0.6744, 0.6768, 0.6765, 0.6735,
        0.6850, 0.6661, 0.6690, 0.6654, 0.6715, 0.6868, 0.6696, 0.6923, 0.6852, 0.6708, 0.6783,
        0.6761
    ]
}

# Data for Llama-7B
llama_7b_data = {
    'step': [
        100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600,
        1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000,
        3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400,
        4500, 4600, 4700, 4800, 4900, 5000, 5100, 5200, 5300, 5400, 5500, 5600, 5700, 5800,
        5900, 6000, 6100, 6200, 6300, 6400, 6500, 6600, 6700, 6800, 6900, 7000, 7100, 7200,
        7300, 7400, 7500, 7600, 7700, 7800, 7900, 8000, 8100, 8200, 8300, 8400, 8500, 8600,
        8700, 8800, 8900, 9000, 9100, 9200, 9300, 9400, 9500, 9600, 9700, 9800, 9900, 10000
    ],
    'loss': [
        1.5932, 1.3830, 1.4630, 1.3655, 1.4481, 1.3772, 1.3219, 1.3056, 1.2647, 1.3174, 1.2218,
        1.2910, 1.2692, 1.2165, 1.2719, 1.2471, 1.2385, 1.1578, 1.1770, 1.1415, 1.0637, 1.0551,
        1.0840, 1.0562, 1.0062, 1.0155, 0.9850, 0.9603, 0.9312, 0.8985, 0.8769, 0.8662, 0.8474,
        0.8447, 0.8241, 0.8300, 0.8263, 0.7959, 0.7804, 0.7879, 0.7582, 0.7725, 0.7534, 0.7835,
        0.7598, 0.7691, 0.7397, 0.7234, 0.7469, 0.7523, 0.7439, 0.7378, 0.7427, 0.7330, 0.7325,
        0.7443, 0.7387, 0.7419, 0.7178, 0.7548, 0.7479, 0.7597, 0.7395, 0.7430, 0.7608, 0.7432,
        0.7433, 0.7549, 0.7425, 0.7520, 0.7552, 0.7584, 0.7519, 0.7481, 0.7628, 0.7649, 0.7484,
        0.7793, 0.7508, 0.7492, 0.7451, 0.7361, 0.7710, 0.7435, 0.7424, 0.7651, 0.7521, 0.7664,
        0.7467, 0.7580, 0.7727, 0.7720, 0.7466, 0.7722, 0.7601, 0.7617, 0.7591, 0.7669, 0.7816,
        0.7749
    ]
}

# Data for Mistral-7B
mistral_7b_data = {
    'step': [
        100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600,
        1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000,
        3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400,
        4500, 4600, 4700, 4800, 4900, 5000, 5100, 5200, 5300, 5400, 5500, 5600, 5700, 5800,
        5900, 6000, 6100, 6200, 6300, 6400, 6500, 6600, 6700, 6800, 6900, 7000, 7100, 7200,
        7300, 7400, 7500, 7600, 7700, 7800, 7900, 8000, 8100, 8200, 8300, 8400, 8500, 8600,
        8700, 8800, 8900, 9000, 9100, 9200, 9300, 9400, 9500, 9600, 9700, 9800, 9900, 10000
    ],
    'loss': [
        1.3777, 1.2000, 1.1991, 1.1939, 1.1724, 1.1952, 1.1740, 1.1665, 1.1815, 1.1598, 1.1447,
        1.1615, 1.1666, 1.1698, 1.1323, 1.1674, 1.1250, 1.1572, 1.1483, 1.1459, 1.0426, 1.0686,
        1.0584, 1.0739, 1.0678, 1.0828, 1.0563, 1.0573, 1.0571, 1.0518, 1.0557, 1.0725, 1.0510,
        1.0714, 1.0562, 1.0781, 1.0651, 1.0657, 1.0396, 1.0583, 0.9382, 0.9603, 0.9575, 0.9524,
        0.9476, 0.9335, 0.9638, 0.9369, 0.9621, 0.9593, 0.9345, 0.9601, 0.9311, 0.9489, 0.9328,
        0.9354, 0.9795, 0.9561, 0.9307, 0.9265, 0.8278, 0.8361, 0.8223, 0.8260, 0.8313, 0.8253,
        0.8294, 0.8394, 0.8226, 0.8413, 0.8206, 0.8157, 0.8241, 0.8232, 0.8307, 0.8301, 0.8349,
        0.8275, 0.8147, 0.8418, 0.7439, 0.7463, 0.7473, 0.7410, 0.7374, 0.7451, 0.7443, 0.7586,
        0.7356, 0.7496, 0.7473, 0.7526, 0.7373, 0.7498, 0.7560, 0.7344, 0.7414, 0.7457, 0.7333,
        0.7575
    ]
}

# Convert to DataFrames
df_flan_t5_large = pd.DataFrame(flan_t5_large_data)
df_llama_7b = pd.DataFrame(llama_7b_data)
df_mistral_7b = pd.DataFrame(mistral_7b_data)

# Plotting
plt.figure(figsize=(14, 8))

plt.plot(df_flan_t5_large['step'], df_flan_t5_large['loss'], label='Flan-T5-Large', color='blue')
plt.plot(df_llama_7b['step'], df_llama_7b['loss'], label='Llama-7B', color='green')
plt.plot(df_mistral_7b['step'], df_mistral_7b['loss'], label='Mistral-7B', color='red')

plt.xlabel('Steps')
plt.ylabel('Loss')
plt.title('Training Loss Across Steps for Various LLMs on the Meadow-MedQA Dataset')
plt.legend()
plt.grid(True)
plt.tight_layout()

# Show plot
plt.show()
